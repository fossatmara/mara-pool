# SRI Pool Configuration with Vitess + Prometheus Metrics
#
# This example demonstrates the composite persistence backend with:
# - Vitess for durable, horizontally-scalable share persistence
# - Prometheus for real-time metrics and monitoring
# - File backend as fallback for both (high reliability)
#
# Architecture:
# Share Events → Vitess (primary) → File (fallback if Vitess fails)
#            ↓
#            → Metrics (real-time) → File (fallback if metrics fails)

# Pool authority keys (generate using pool-sv2 keygen)
authority_public_key = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
authority_secret_key = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n"

# Certificate validity in seconds
cert_validity_sec = 3600

# Pool listening address
listen_address = "0.0.0.0:3333"

# Coinbase reward output script
coinbase_reward_script = "addr(bc1qtzqxqaxyy6lda2fhdtp5dp0v56vlf6g0tljy2x)"

# Server identifier
server_id = 1

# Pool signature displayed to miners
pool_signature = "Stratum V2 SRI Pool"

# Target difficulty - shares per minute per miner
shares_per_minute = 6.0

# Share batch size for processing
share_batch_size = 10

# Supported and required protocol extensions
supported_extensions = []
required_extensions = []

# Template provider configuration
[template_provider_type.Sv2Tp]
address = "127.0.0.1:8442"

# =============================================================================
# Composite Persistence Configuration: Vitess + Metrics + File Fallback
# =============================================================================
#
# The composite backend automatically routes events to multiple destinations:
# 1. Vitess - Durable storage with horizontal sharding
# 2. Metrics - Real-time Prometheus metrics for monitoring
# 3. File - Fallback destination if either Vitess or Metrics fails
#
# Each backend handles the entity types specified in its 'entities' list.
# Fallback chains ensure no data loss even during infrastructure failures.
#

# -----------------------------------------------------------------------------
# Vitess Backend - Primary Durable Storage
# -----------------------------------------------------------------------------
[persistence.vitess]
# Vitess connection string (MySQL protocol via vtgate)
# Format: mysql://username:password@vtgate_host:port/keyspace
connection_string = "mysql://pool_app:secure_password@vtgate-lb.prod.internal:15306/mining_pool"

# Connection pool configuration
pool_size = 20                    # Concurrent connections to vtgate

# Event queueing (mining hot path -> worker)
channel_size = 10000              # Non-blocking buffer size

# Batch insert configuration
batch_size = 100                  # Events per database transaction
batch_timeout_ms = 1000           # Max wait before flushing partial batch (1 second)

# Retry and resilience
retry_max_attempts = 3            # Retries before using fallback
retry_timeout_secs = 10           # Cooldown between retry attempts

# Internal fallback (for retry queue overflow)
# This is separate from the composite backend fallback chain
fallback_file_path = "/var/log/pool/vitess_retry_overflow.log"

# Entity routing
entities = ["shares"]             # This backend handles share events

# Composite fallback chain
fallback = "file"                 # If Vitess fails, route to file backend

# -----------------------------------------------------------------------------
# Metrics Backend - Real-Time Prometheus Metrics
# -----------------------------------------------------------------------------
[persistence.metrics]
# HTTP endpoint for Prometheus scraping
resource_path = "/metrics"

# Port for metrics HTTP server
port = 9091

# Entity routing
entities = ["shares"]             # This backend handles share events

# Composite fallback chain
fallback = "file"                 # If metrics server fails, route to file backend

# -----------------------------------------------------------------------------
# File Backend - Fallback and Audit Trail
# -----------------------------------------------------------------------------
[persistence.file]
# File path for fallback persistence
file_path = "/var/log/pool/share_events.log"

# Channel buffer size
channel_size = 10000

# Entity routing
entities = ["shares"]             # This backend handles share events

# No fallback - file is the last resort

# =============================================================================
# How This Configuration Works
# =============================================================================
#
# Event Flow:
# 1. Miner submits share
# 2. Share validated and PersistenceEvent created
# 3. Composite backend routes event to ALL configured backends in parallel:
#
#    ┌─> Vitess Backend
#    │   ├─ Success: Event stored in sharded database
#    │   └─ Failure: Retry queue (up to 3 attempts)
#    │              └─ Max retries: Route to file backend
#    │
#    ├─> Metrics Backend
#    │   ├─ Success: Prometheus counters/gauges updated
#    │   └─ Failure: Route to file backend
#    │
#    └─> File Backend (receives events from fallback chains)
#        ├─ Success: Event appended to log file
#        └─ Failure: Logged error (last resort)
#
# Benefits:
# - Zero data loss: Multiple storage destinations + fallback chains
# - Real-time monitoring: Prometheus metrics available immediately
# - Audit trail: File backend captures all events and fallback events
# - High availability: System continues even if Vitess or metrics fail
#
# =============================================================================
# Performance Characteristics
# =============================================================================
#
# Expected Throughput:
# - Vitess: 1000+ shares/sec sustained
# - Metrics: 1000+ shares/sec (in-memory counters)
# - File: 1000+ shares/sec (async append)
#
# Memory Footprint:
# - Vitess: ~15MB (channel + retry queue + batch buffers)
# - Metrics: ~5MB (in-memory Prometheus registry)
# - File: ~5MB (channel buffer)
# - Total: ~25MB for all persistence backends
#
# Latency (P99):
# - Share submission: < 50ms (non-blocking guarantee)
# - Vitess batch write: 50-100ms (async, doesn't block mining)
# - Metrics update: < 1ms (in-memory)
# - File write: 10-20ms (async)
#
# =============================================================================
# Monitoring Setup
# =============================================================================
#
# Prometheus scrape config:
#
# scrape_configs:
#   - job_name: 'sv2_pool'
#     static_configs:
#       - targets: ['pool-server:9091']
#
# Key Metrics Available:
# - sv2_pool_shares_total{status="valid|invalid|stale"}
# - sv2_pool_blocks_found_total
# - sv2_pool_hashrate_estimate
# - sv2_pool_connected_miners
# - sv2_pool_share_difficulty_sum
#
# Grafana Dashboard:
# Import monitoring/grafana/provisioning/dashboards/json/sv2-pool-overview.json
#
# =============================================================================
# Deployment Checklist
# =============================================================================
#
# Before starting the pool:
# ☐ Vitess cluster deployed and accessible
# ☐ Database schema applied (docs/vitess/schema.sql)
# ☐ VSchema configured (docs/vitess/vschema.json)
# ☐ Prometheus server deployed and scraping port 9091
# ☐ Grafana dashboard imported (optional)
# ☐ File backend directory exists and is writable
# ☐ Fallback file path is writable
#
# Build command:
# cargo build --release --features "vitess,metrics"
#
# =============================================================================
# Troubleshooting
# =============================================================================
#
# Issue: High retry queue in Vitess
# → Check vtgate connectivity and logs
# → Monitor fallback file growth
# → Scale vtgate if needed
#
# Issue: Metrics not updating
# → Check port 9091 is accessible
# → Verify Prometheus scrape config
# → Check metrics backend fallback to file
#
# Issue: File backend growing rapidly
# → Indicates primary backends (Vitess/Metrics) are failing
# → Check infrastructure health
# → Review application logs for errors
#
